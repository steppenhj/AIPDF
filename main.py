# app.py â€” PDF ê¸°ë°˜ Q&A (ê°•í™” RAG, ë¹„ìš© ìµœì í™” + ê²¬ê³  ì¶œë ¥, ëŒ€í™” ê¸°ëŠ¥ ì¶”ê°€)
import os, re, json, tempfile, hashlib, pathlib
from typing import List, Tuple

from dotenv import load_dotenv
load_dotenv(override=True)

import streamlit as st
import pandas as pd

# LangChain / OpenAI
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
try:
    from langchain_community.document_loaders import PyPDFLoader
except ModuleNotFoundError:
    from langchain_community.document_loaders.pdf import PyPDFLoader  # ì•„ì£¼ êµ¬ë²„ì „ ëŒ€ë¹„

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import HumanMessage, AIMessage


# (ì„ íƒ) ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# =========================
# ê¸°ë³¸ UI
# =========================
st.set_page_config(page_title="PDF QA (ê°•í™” RAG + ëŒ€í™”)", page_icon="ğŸ“„", layout="centered")
st.title("ğŸ“„ PDF ê¸°ë°˜ Q&A (ê°•í™” RAG + ëŒ€í™” ê¸°ëŠ¥)")

# =========================
# API í‚¤
# =========================
def get_openai_key():
    key = os.getenv("OPENAI_API_KEY")
    if key:
        return key
    try:
        return st.secrets["OPENAI_API_KEY"]
    except Exception:
        return None

OPENAI_API_KEY = get_openai_key()
if not OPENAI_API_KEY:
    st.error("OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. (.env ë˜ëŠ” .streamlit/secrets.toml / í´ë¼ìš°ë“œ Secrets ì„¤ì •)")
    st.stop()

# ===== ëª¨ë¸ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥) =====
PRIMARY_MODEL = os.getenv("OPENAI_CHAT_MODEL_PRIMARY", "gpt-4o-mini")  # ë©”ì¸ ìƒì„±/ì¶”ë¡ 
LIGHT_MODEL   = os.getenv("OPENAI_CHAT_MODEL_LIGHT",   "gpt-4o-mini")  # ì••ì¶•/ì •ì œ/ê°„ë‹¨ íƒœìŠ¤í¬ (ìµœì‹  ë¯¸ë‹ˆ ëª¨ë¸ë¡œ í†µì¼)
EMBED_MODEL   = os.getenv("OPENAI_EMBED_MODEL",        "text-embedding-3-small")  # ìµœì €ê°€ ì„ë² ë”©

# =========================
# ë¹„ìš© ìµœì í™” ê¸°ë³¸ê°’ (ìŠ¬ë¼ì´ë”)
# =========================
with st.sidebar:
    st.subheader("ğŸ”§ ì¸ë±ì‹±")
    chunk_size = st.slider("Chunk size", 300, 2000, 900, 50)
    chunk_overlap = st.slider("Chunk overlap", 0, 400, 150, 10)
    st.caption("ë„ˆë¬´ ì‘ìœ¼ë©´ ì²­í¬ ìˆ˜â†‘(ì„ë² ë”© ë¹„ìš©â†‘), ë„ˆë¬´ í¬ë©´ ê²€ìƒ‰ ì •ë°€ë„â†“")

    st.subheader("ğŸ” ê²€ìƒ‰/ìƒì„±")
    top_k = st.slider("ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ k", 1, 10, 5, 1)
    fetch_k = st.slider("í›„ë³´ fetch_k", 10, 60, 20, 2)
    use_compression = st.checkbox("ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì‚¬ìš©(ê¶Œì¥, ë¹„ìš©â†“Â·ì •í™•ë„â†‘)", True)
    temperature = st.slider("temperature", 0.0, 1.0, 0.2, 0.1)
    max_tokens = st.slider("ì‘ë‹µ í† í° ìƒí•œ", 200, 1200, 450, 50)

    st.subheader("ğŸ§  ë¹„íŒ/ë¦¬ìŠ¤í¬ ì§ˆë¬¸ ì²˜ë¦¬(ì „ì—­ ìŠ¤ìº”)")
    enable_global_critique = st.checkbox("ë¹„íŒí˜• ì§ˆë¬¸ì— ì „ì—­ ìŠ¤ìº” ì‚¬ìš©", True)
    global_critique_pages = st.slider("ì „ì—­ ìŠ¤ìº”: í˜ì´ì§€ë‹¹ ë°œì·Œ ê¸¸ì´", 200, 1200, 700, 50)
    global_critique_total = st.slider("ì „ì—­ ìŠ¤ìº”: ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´", 4000, 20000, 12000, 500)

# =========================
# ìœ í‹¸ (ì˜ë„Â·ê°œìˆ˜ íŒŒì‹± / ë¹„ìš©Â·ìºì‹œ)
# =========================
PROS_KEYS = ["ì˜ëœ", "ì˜ ëœ", "ì¥ì ", "ê°•ì ", "ì¢‹ì€ ì "]
CONS_KEYS = ["ë¶€ì¡±", "ë‹¨ì ", "í•œê³„", "ë¦¬ìŠ¤í¬", "ë¬¸ì œì ", "ì·¨ì•½", "ë¶ˆí¸", "ì œì•½", "ìœ„í—˜", "ë³´ì™„"]

def extract_count(q: str, default=3) -> int:
    m = re.search(r"(\d+)\s*ê°€ì§€", q or "")
    return int(m.group(1)) if m else default

def detect_intent(q: str):
    ql = (q or "").lower()
    has_pros = any(k in ql for k in [*PROS_KEYS, "pros", "advantages"])
    has_cons = any(k in ql for k in [*CONS_KEYS, "cons", "risks"])
    if has_pros and has_cons: return "pros_cons"  # ì¥/ë‹¨ì  í˜¼í•©
    if has_cons: return "critique"                # ë‹¨ì /ë¦¬ìŠ¤í¬ ì¤‘ì‹¬
    return "general"

def looks_like_critique(q: str) -> bool:
    return detect_intent(q) == "critique"

def sha256_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()[:16]

def save_dir_for(hash_key: str) -> str:
    base = pathlib.Path(".faiss_cache")
    base.mkdir(parents=True, exist_ok=True)
    return str(base / f"vs_{hash_key}")

def extract_page_citations(text: str) -> List[int]:
    pages = set(int(p) for p in re.findall(r"\[p\.(\d+)\]", text or ""))
    return sorted(list(pages))

# ---- ê²¬ê³ í•œ JSON íŒŒì„œ ----
def safe_json_loads(s: str, allow_empty: bool = False):
    if not s or not s.strip():
        if allow_empty:
            return {}
        raise ValueError("empty JSON response")
    s = s.strip()
    if s.startswith("```"):
        s = re.sub(r"^```(?:json)?\s*|\s*```$", "", s, flags=re.DOTALL)
    m = re.search(r"\{[\s\S]*\}", s)
    if m: s = m.group(0)
    if "'" in s and '"' not in s:
        s = s.replace("'", '"')
    s = re.sub(r"(?m)^\s*//.*$", "", s)
    try:
        return json.loads(s)
    except Exception:
        if allow_empty:
            return {}
        raise

# ---- LLM ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ê°•ì œ ì¶”ì¶œ (tool_calls ëŒ€ë¹„) ----
def to_text(resp) -> str:
    """
    BaseMessageì—ì„œ contentê°€ ë¹„ì—ˆì„ ë•Œ tool_calls/function_call.argumentsì—ì„œ ë³¸ë¬¸ íšŒìˆ˜.
    í•­ìƒ ë¬¸ìì—´ ë°˜í™˜.
    """
    if resp is None:
        return ""
    try:
        text = (getattr(resp, "content", None) or "").strip()
        if text:
            return text
        ak = getattr(resp, "additional_kwargs", {}) or {}
        # OpenAI tool_calls
        tcs = ak.get("tool_calls") or []
        if tcs:
            fn = (tcs[0] or {}).get("function", {})
            args = fn.get("arguments", "") or ""
            return str(args).strip()
        # legacy function_call
        fc = ak.get("function_call") or {}
        if fc:
            return str(fc.get("arguments", "") or "").strip()
        return ""
    except Exception:
        return ""

# =========================
# ì¸ë±ìŠ¤ ìƒì„±/ë¡œë”© (ìºì‹œ + ë””ìŠ¤í¬ ì €ì¥)
# =========================
@st.cache_resource(show_spinner=False)
def load_or_build_index(pdf_bytes: bytes, chunk_size:int, chunk_overlap:int, api_key:str):
    pdf_hash = sha256_bytes(pdf_bytes)
    folder = save_dir_for(pdf_hash)

    if os.path.isdir(folder) and os.path.exists(os.path.join(folder, "index.faiss")):
        embeddings = OpenAIEmbeddings(api_key=api_key, model=EMBED_MODEL)
        vs = FAISS.load_local(folder, embeddings, allow_dangerous_deserialization=True)
    else:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(pdf_bytes)
            path = tmp.name
        docs = PyPDFLoader(path).load()

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=["\n\n", "\n", " ", ""]
        )
        splits = splitter.split_documents(docs)

        embeddings = OpenAIEmbeddings(api_key=api_key, model=EMBED_MODEL)
        vs = FAISS.from_documents(splits, embeddings)
        vs.save_local(folder)

    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(pdf_bytes)
        path2 = tmp.name
    page_docs = PyPDFLoader(path2).load()
    pages = [(d.metadata.get("page", 0)+1, d.page_content) for d in page_docs]
    return vs, pages

def _format_docs(docs, max_chars=3000):
    text = "\n\n".join(f"[p.{(d.metadata.get('page',0)+1)}] {d.page_content}" for d in docs)
    return text[:max_chars]

# =========================
# LLM í˜¸ì¶œ í—¬í¼ (4o-mini / JSON ëª¨ë“œ)
# =========================
def llm_chat(max_tokens:int=600, temperature:float=0.2):
    return ChatOpenAI(api_key=OPENAI_API_KEY, model=PRIMARY_MODEL,
                      temperature=temperature, max_tokens=max_tokens)

def llm_light(max_tokens:int=300, temperature:float=0):
    return ChatOpenAI(api_key=OPENAI_API_KEY, model=LIGHT_MODEL,
                      temperature=temperature, max_tokens=max_tokens)

def llm_chat_json(max_tokens:int=700, temperature:float=0):
    # json_object ê°•ì œ + ì‹¤íŒ¨ ì‹œ ì¼ë°˜ëª¨ë“œë¡œ JSONë§Œ ì¶œë ¥í•˜ë„ë¡ í´ë°±
    def _invoke(msgs):
        try:
            return ChatOpenAI(
                api_key=OPENAI_API_KEY,
                model=PRIMARY_MODEL,
                temperature=temperature,
                max_tokens=max_tokens,
                model_kwargs={"response_format": {"type": "json_object"}},
            ).invoke(msgs).content
        except Exception:
            hard_sys = (
                'ë‹¤ìŒ ìš”ì²­ì— ëŒ€í•´ {"pros":["..."],"cons":["..."]} í˜•ì‹ì˜ **ìœ íš¨í•œ JSON í•œ ê°œë§Œ** ì¶œë ¥í•˜ë¼. '
                "ê·¸ ì™¸ í…ìŠ¤íŠ¸/ì½”ë“œë¸”ë¡/ì„¤ëª… ê¸ˆì§€."
            )
            return ChatOpenAI(
                api_key=OPENAI_API_KEY,
                model=PRIMARY_MODEL,
                temperature=0,
                max_tokens=max_tokens
            ).invoke(
                [{"role":"system","content":hard_sys}] + msgs
            ).content
    class _Runner:
        def invoke(self, msgs):
            return type("resp", (), {"content": _invoke(msgs)})()
    return _Runner()


# =========================
# pros/cons ì „ìš© ë£¨íŠ¸: JSON â†’ ë§ˆí¬ë‹¤ìš´ (3ì¤‘ ì•ˆì „ë§)
# =========================
def answer_pros_cons(question: str, retriever, pages, n: int, use_global: bool,
                     per_page_chars:int, total_chars:int, api_key:str, max_tokens:int=750) -> str:
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    if use_global and pages:
        buf = [f"[p.{p}] {txt[:per_page_chars]}" for p, txt in pages]
        context = "\n\n".join(buf)[:total_chars]
    else:
        docs = retriever.get_relevant_documents(question)
        context = "\n\n".join(
            f"[p.{d.metadata.get('page',0)+1}] {d.page_content}" for d in docs
        )[: min(12000, total_chars)]

    sys = (
        "ë‹¤ìŒ CONTEXTì— ê·¼ê±°í•´ ì‚¬ìš©ìì˜ ì§ˆë¬¸(ì¥ì ê³¼ ë‹¨ì ì„ ê°ê° Nê°œ)ì„ ì •í™•íˆ ë”°ë¥¸ë‹¤. "
        "ê° í•­ëª©ì˜ ë¬¸ì¥ ëì— ë°˜ë“œì‹œ [p.x] ê·¼ê±°ë¥¼ í¬í•¨í•œë‹¤. "
        'ì¶œë ¥ì€ {"pros":["... [p.x]"],"cons":["... [p.y]"]} í˜•ì‹ì˜ JSON **í•œ ê°œ**ë§Œ ë°˜í™˜í•˜ë¼.'
    )
    user = f"N={n}\nì§ˆë¬¸: {question}\n\nCONTEXT:\n{context}"

    # 1) JSON ëª¨ë“œ 1ì°¨ + ë¯¸ì„¸ ì¬ì‹œë„
    data = {}
    out = ""
    for _ in range(2):
        out = to_text(
            llm_chat_json(max_tokens=max_tokens, temperature=0).invoke(
                [{"role":"system","content":sys},{"role":"user","content":user}]
            )
        )
        data = safe_json_loads(out, allow_empty=True)
        if data:
            break

    # 2) ê²½ëŸ‰(nano)ë¡œ JSON ì¶”ì¶œ/ìˆ˜ì •
    if not data:
        out2 = to_text(
            llm_light(max_tokens=300, temperature=0).invoke(
                [
                    {"role":"system","content":"ì•„ë˜ í…ìŠ¤íŠ¸ì—ì„œ í•„ìš”í•œ JSONë§Œ ì¶”ì¶œ/ìˆ˜ì •í•˜ì—¬ ìœ íš¨í•œ JSON í•œ ê°œë¡œ ë°˜í™˜í•˜ë¼."},
                    {"role":"user","content":out or ""},
                ]
            )
        )
        data = safe_json_loads(out2, allow_empty=True)

    # 3) ê·¸ë˜ë„ ì‹¤íŒ¨ â†’ ë§ˆí¬ë‹¤ìš´ ì§ì ‘ ìƒì„±
    if not isinstance(data, dict) or ("pros" not in data and "cons" not in data):
        md = to_text(
            llm_chat(max_tokens=max_tokens).invoke(
                [
                    {"role":"system","content":f"ì¥ì  {n}ê°œì™€ ë‹¨ì  {n}ê°œë¥¼ ê°ê° bulletë¡œ ìƒì„±í•˜ë¼. ê° bullet ëì— [p.x]ë¥¼ ë¶™ì—¬ë¼. ë¬¸ì„œ ì™¸ ì¶”ì¸¡ ê¸ˆì§€."},
                    {"role":"user","content":f"ì§ˆë¬¸: {question}\n\nCONTEXT:\n{context}"},
                ]
            )
        )
        return md

    pros = list(data.get("pros", []))[:n]
    cons = list(data.get("cons", []))[:n]
    if not pros: pros = ["ë¬¸ì„œ ê¸°ë°˜ ì¥ì  ìš”ì•½ [p.?]"]
    if not cons: cons = ["ë¬¸ì„œ ê¸°ë°˜ ë¶€ì¡±/ë¦¬ìŠ¤í¬ ìš”ì•½ [p.?]"]

    return "\n".join([
        "#### âœ… ì˜ëœ ì ", *[f"- {p}" for p in pros], "",
        "#### âš ï¸ ë¶€ì¡±í•œ ì ", *[f"- {c}" for c in cons]
    ])

# =========================
# ì „ì—­ ìŠ¤ìº”(ë¹„íŒí˜•): ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë”°ë¦„
# =========================
def critique_answer_global(pages: List[Tuple[int,str]], per_page_chars:int, total_chars:int,
                           api_key:str, question:str, max_tokens:int=700) -> str:
    buf = [f"[p.{p}] {txt[:per_page_chars]}" for p, txt in pages]
    context = "\n\n".join(buf)[:total_chars]
    sys = (
        "ë‹¤ìŒ CONTEXTì— ê·¼ê±°í•´ **ì‚¬ìš©ì ì§ˆë¬¸ì„ ì •í™•íˆ ë”°ë¥¸ë‹¤**. "
        "ê°€ëŠ¥í•˜ë©´ bulletë¡œ ê°„ê²°íˆ ì„œìˆ í•˜ê³ , ê° í•­ëª© ëì— [p.x]ë¥¼ ë¶™ì¸ë‹¤. "
        "ë¬¸ì„œ ì™¸ ì¶”ì¸¡ ê¸ˆì§€."
    )
    user = f"ì§ˆë¬¸: {question}\n\nCONTEXT:\n{context}"
    return to_text(
        llm_chat(max_tokens=max_tokens).invoke(
            [{"role":"system","content":sys},{"role":"user","content":user}]
        )
    )

# =========================
# ìƒíƒœ ì´ˆê¸°í™”
# =========================
if "vs" not in st.session_state: st.session_state.vs = None
if "pages" not in st.session_state: st.session_state.pages = None
if "messages" not in st.session_state: st.session_state.messages = [] # ëŒ€í™” ê¸°ë¡
if "uploaded_name" not in st.session_state: st.session_state.uploaded_name = None

# =========================
# PDF ì—…ë¡œë“œ ì²˜ë¦¬
# =========================
uploaded = st.file_uploader("PDF ì—…ë¡œë“œ", type=["pdf"])

if uploaded is not None:
    if st.session_state.uploaded_name != uploaded.name:
        with st.spinner("PDF ì¸ë±ì‹±/ìºì‹œ ì¤€ë¹„ ì¤‘..."):
            vs, pages = load_or_build_index(uploaded.read(), chunk_size, chunk_overlap, OPENAI_API_KEY)
            st.session_state.vs = vs
            st.session_state.pages = pages
            st.session_state.uploaded_name = uploaded.name
            st.session_state.messages = []  # ìƒˆ íŒŒì¼ ì—…ë¡œë“œ ì‹œ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
        st.success(f"'{uploaded.name}' ì¸ë±ì‹± ì™„ë£Œ! ì´ì œ ì§ˆë¬¸ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
else:
    # íŒŒì¼ì´ ì œê±°ë˜ë©´ ê´€ë ¨ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    st.session_state.vs = None
    st.session_state.pages = None
    st.session_state.uploaded_name = None
    st.session_state.messages = []

# =========================
# ëŒ€í™”í˜• ì²´ì¸ ë¹Œë” (ëŒ€í™” ê¸°ë¡ í¬í•¨)
# =========================
def build_chain(retriever):
    system = (
        "ë„ˆëŠ” ì—…ë¡œë“œëœ PDFì— ê·¼ê±°í•´ í•œêµ­ì–´ë¡œ ë‹µí•œë‹¤. "
        "ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, í˜„ì¬ ì§ˆë¬¸ì— ê°€ì¥ ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•œë‹¤. "
        "í•­ìƒ **ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ì™€ ê°œìˆ˜ ìš”êµ¬ë¥¼ ì •í™•íˆ ë”°ë¥¸ë‹¤**. "
        "ê°€ëŠ¥í•˜ë©´ bulletì„ ì‚¬ìš©í•˜ê³ , ê° í•µì‹¬ ì£¼ì¥ ëì— [p.í˜ì´ì§€] ê·¼ê±°ë¥¼ ë¶™ì¸ë‹¤. "
        "ë¬¸ì„œì™€ ë¬´ê´€í•œ ì¶”ì¸¡ì€ ê¸ˆì§€í•œë‹¤."
    )
    prompt = ChatPromptTemplate.from_messages([
        ("system", system + "\n\nCONTEXT:\n{context}"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}")
    ])
    chain = (
        {
            "context": retriever | _format_docs,
            "question": RunnablePassthrough(),
            "chat_history": RunnablePassthrough() # invoke ì‹œì ì— chat_historyë¥¼ ì „ë‹¬
        }
        | prompt
        | llm_chat(max_tokens=max_tokens, temperature=temperature)
        | StrOutputParser()
    )
    return chain


# =========================
# Q&A ë° ëŒ€í™” ê¸°ë¡ UI
# =========================
if st.session_state.vs:
    st.markdown("---")

    # ì´ì „ ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
    if question := st.chat_input("PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”... (ì˜ˆ: 'ì˜ëœ ì  3ê°€ì§€ì™€ ë¶€ì¡±í•œ ì  3ê°€ì§€ë¥¼ ì•Œë ¤ì¤˜')"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê¸°ë¡í•˜ê³  í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)

        # AI ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                intent = detect_intent(question)
                n = extract_count(question, 3)

                # ë¦¬íŠ¸ë¦¬ë²„ (MMR + ì••ì¶•)
                base_retriever = st.session_state.vs.as_retriever(
                    search_type="mmr", search_kwargs={"k": top_k, "fetch_k": fetch_k}
                )
                retriever = base_retriever
                if use_compression:
                    compressor = LLMChainExtractor.from_llm(
                        llm_light(max_tokens=300, temperature=0) # ì••ì¶•ì€ ê²½ëŸ‰ ëª¨ë¸ë¡œ ë¹„ìš© ì ˆê°
                    )
                    retriever = ContextualCompressionRetriever(
                        base_retriever=base_retriever, base_compressor=compressor
                    )

                # ì§ˆë¬¸ ì˜ë„ì— ë”°ë¼ ë¼ìš°íŒ…
                if intent == "pros_cons":
                    answer = answer_pros_cons(
                        question=question,
                        retriever=retriever,
                        pages=st.session_state.pages,
                        n=n,
                        use_global=enable_global_critique,
                        per_page_chars=global_critique_pages,
                        total_chars=global_critique_total,
                        api_key=OPENAI_API_KEY,
                        max_tokens=max_tokens + 250
                    )
                elif intent == "critique" and enable_global_critique and st.session_state.pages:
                    answer = critique_answer_global(
                        st.session_state.pages,
                        per_page_chars=global_critique_pages,
                        total_chars=global_critique_total,
                        api_key=OPENAI_API_KEY,
                        question=question,
                        max_tokens=max_tokens + 150
                    )
                else:
                    # ì¼ë°˜ ì§ˆë¬¸ì€ ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•˜ì—¬ ì²˜ë¦¬
                    chain = build_chain(retriever)
                    # LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    chat_history = []
                    for msg in st.session_state.messages[:-1]: # ë§ˆì§€ë§‰ user ë©”ì‹œì§€ëŠ” ì œì™¸
                        if msg["role"] == "user":
                            chat_history.append(HumanMessage(content=msg["content"]))
                        elif msg["role"] == "assistant":
                            chat_history.append(AIMessage(content=msg["content"]))

                    answer = chain.invoke({
                        "question": question,
                        "chat_history": chat_history
                    })

                # ë¹ˆ ë¬¸ìì—´ ê°€ë“œ
                if not (answer or "").strip():
                    answer = "âš ï¸ ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ í† í° í•œë„ë¥¼ ë‚®ì¶°ë³´ì„¸ìš”."
                st.write(answer)

                # ê·¼ê±° ìŠ¤ë‹ˆí« ë¯¸ë¦¬ë³´ê¸°
                cited_pages = extract_page_citations(answer)
                if cited_pages and st.session_state.pages:
                    with st.expander(f"ğŸ” ì¸ìš©ëœ í˜ì´ì§€ ìŠ¤ë‹ˆí« ë³´ê¸° ({len(cited_pages)}ê°œ)"):
                        page_map = {pg: txt for pg, txt in st.session_state.pages}
                        for pg in cited_pages:
                            snippet = (page_map.get(pg) or "")[:500]
                            st.markdown(f"**[p.{pg}]**\n\n{snippet}")

        # AI ë©”ì‹œì§€ë¥¼ ê¸°ë¡
        st.session_state.messages.append({"role": "assistant", "content": answer})

else:
    st.info("PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ ì§ˆë¬¸ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")