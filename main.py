# app.py â€” PDF ê¸°ë°˜ Q&A (UI ë‹¨ìˆœí™” ë²„ì „)
import os, re, json, tempfile, hashlib, pathlib
from typing import List, Tuple
from operator import itemgetter

from dotenv import load_dotenv
load_dotenv(override=True)

import streamlit as st
import pandas as pd
import altair as alt

# LangChain / OpenAI
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
try:
    from langchain_community.document_loaders import PyPDFLoader
except ModuleNotFoundError:
    from langchain_community.document_loaders.pdf import PyPDFLoader

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import HumanMessage, AIMessage

from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# =========================
# ê¸°ë³¸ UI ë° í˜ì´ì§€ ì„¤ì •
# =========================
st.set_page_config(page_title="PDF Q&A (Simple UI)", page_icon="ğŸ“„", layout="centered")
st.title("ğŸ“„ PDF ê¸°ë°˜ AI Q&A")

# =========================
# >>> NEW: ë‚´ë¶€ í‘œì¤€ ì„¤ì •ê°’ (ì‚¬ì´ë“œë°” ì œê±°) <<<
# =========================
# ì¸ë±ì‹± ì„¤ì •
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 150
# ê²€ìƒ‰/ìƒì„± ì„¤ì •
TOP_K = 6
FETCH_K = 20
USE_COMPRESSION = True
TEMPERATURE = 0.1
MAX_TOKENS = 1000
# ë¹„íŒ/ë¦¬ìŠ¤í¬ ì§ˆë¬¸ ì²˜ë¦¬(ì „ì—­ ìŠ¤ìº”) ì„¤ì •
ENABLE_GLOBAL_CRITIQUE = True
GLOBAL_CRITIQUE_PAGES_CHARS = 700
GLOBAL_CRITIQUE_TOTAL_CHARS = 12000

# =========================
# API í‚¤
# =========================
def get_openai_key():
    key = os.getenv("OPENAI_API_KEY")
    if key: return key
    try: return st.secrets["OPENAI_API_KEY"]
    except Exception: return None

OPENAI_API_KEY = get_openai_key()
if not OPENAI_API_KEY:
    st.error("OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. (.env ë˜ëŠ” .streamlit/secrets.toml / í´ë¼ìš°ë“œ Secrets ì„¤ì •)")
    st.stop()

# ===== ëª¨ë¸ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥) =====
PRIMARY_MODEL = os.getenv("OPENAI_CHAT_MODEL_PRIMARY", "gpt-4o-mini")
LIGHT_MODEL   = os.getenv("OPENAI_CHAT_MODEL_LIGHT",   "gpt-4o-mini")
EMBED_MODEL   = os.getenv("OPENAI_EMBED_MODEL",        "text-embedding-3-small")

# ... (ì´ì „ ì½”ë“œì˜ ìœ í‹¸ë¦¬í‹°, ì¸ë±ì‹±, LLM í—¬í¼, ë‹µë³€ ìƒì„±ê¸° í•¨ìˆ˜ë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€) ...
# =========================
# ìœ í‹¸ (ì˜ë„Â·ê°œìˆ˜ íŒŒì‹± / ë¹„ìš©Â·ìºì‹œ)
# =========================
PROS_KEYS = ["ì˜ëœ", "ì˜ ëœ", "ì¥ì ", "ê°•ì ", "ì¢‹ì€ ì "]
CONS_KEYS = ["ë¶€ì¡±", "ë‹¨ì ", "í•œê³„", "ë¦¬ìŠ¤í¬", "ë¬¸ì œì ", "ì·¨ì•½", "ë¶ˆí¸", "ì œì•½", "ìœ„í—˜", "ë³´ì™„"]
def extract_count(q: str, default=3) -> int:
    m = re.search(r"(\d+)\s*ê°€ì§€", q or "")
    return int(m.group(1)) if m else default
def detect_intent(q: str):
    ql = (q or "").lower()
    has_pros = any(k in ql for k in [*PROS_KEYS, "pros", "advantages"])
    has_cons = any(k in ql for k in [*CONS_KEYS, "cons", "risks"])
    if has_pros and has_cons: return "pros_cons"
    if has_cons: return "critique"
    return "general"
def sha256_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()[:16]
def save_dir_for(hash_key: str) -> str:
    base = pathlib.Path(".faiss_cache")
    base.mkdir(parents=True, exist_ok=True)
    return str(base / f"vs_{hash_key}")
def extract_page_citations(text: str) -> List[int]:
    pages = set(int(p) for p in re.findall(r"\[p\.(\d+)\]", text or ""))
    return sorted(list(pages))
def to_text(resp) -> str:
    if resp is None: return ""
    try:
        text = (getattr(resp, "content", None) or "").strip()
        if text: return text
        ak = getattr(resp, "additional_kwargs", {}) or {}
        tcs = ak.get("tool_calls") or []
        if tcs:
            fn = (tcs[0] or {}).get("function", {}); args = fn.get("arguments", "") or ""
            return str(args).strip()
        fc = ak.get("function_call") or {}
        if fc: return str(fc.get("arguments", "") or "").strip()
        return ""
    except Exception: return ""
# =========================
# ì¸ë±ìŠ¤ ìƒì„±/ë¡œë”© (ìºì‹œ + ë””ìŠ¤í¬ ì €ì¥)
# =========================
@st.cache_resource(show_spinner=False)
def load_or_build_index(pdf_bytes: bytes, chunk_size:int, chunk_overlap:int, api_key:str):
    pdf_hash = sha256_bytes(pdf_bytes)
    folder = save_dir_for(pdf_hash)
    if os.path.isdir(folder) and os.path.exists(os.path.join(folder, "index.faiss")):
        embeddings = OpenAIEmbeddings(api_key=api_key, model=EMBED_MODEL)
        vs = FAISS.load_local(folder, embeddings, allow_dangerous_deserialization=True)
    else:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(pdf_bytes); path = tmp.name
        docs = PyPDFLoader(path).load()
        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=["\n\n", "\n", " ", ""])
        splits = splitter.split_documents(docs)
        embeddings = OpenAIEmbeddings(api_key=api_key, model=EMBED_MODEL)
        vs = FAISS.from_documents(splits, embeddings)
        vs.save_local(folder)
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(pdf_bytes); path2 = tmp.name
    page_docs = PyPDFLoader(path2).load()
    pages = [(d.metadata.get("page", 0)+1, d.page_content) for d in page_docs]
    return vs, pages
def _format_docs(docs, max_chars=4000):
    return "\n\n".join(f"[p.{(d.metadata.get('page',0)+1)}] {d.page_content}" for d in docs)[:max_chars]
# =========================
# LLM í˜¸ì¶œ í—¬í¼
# =========================
def llm_chat(max_tokens:int=600, temperature:float=0.2):
    return ChatOpenAI(api_key=OPENAI_API_KEY, model=PRIMARY_MODEL, temperature=temperature, max_tokens=max_tokens)
def llm_light(max_tokens:int=300, temperature:float=0):
    return ChatOpenAI(api_key=OPENAI_API_KEY, model=LIGHT_MODEL, temperature=temperature, max_tokens=max_tokens)
def llm_chat_json(model=PRIMARY_MODEL, max_tokens=700, temperature=0.1):
    return ChatOpenAI(
        api_key=OPENAI_API_KEY, model=model, temperature=temperature, max_tokens=max_tokens,
        model_kwargs={"response_format": {"type": "json_object"}},
    )
# =========================
# AI ì§ˆë¬¸ ì¶”ì²œ ê¸°ëŠ¥
# =========================
@st.cache_data(show_spinner="ë¬¸ì„œ ë¶„ì„ ë° ì§ˆë¬¸ ì¶”ì²œ ì¤‘...")
def generate_question_suggestions(_pages: List[Tuple[int,str]]) -> List[str]:
    context = "\n".join([f"[p.{p}] {txt[:500]}" for p, txt in _pages[:5]])[:4000]
    sys = (
        "You are an AI assistant that helps users understand complex documents. "
        "Based on the provided context from a document, generate three insightful and distinct questions a user might ask. "
        'Return the result as a valid JSON list of strings. Example: ["Question 1?", "Question 2?", "Question 3?"]'
    )
    user = f"CONTEXT:\n{context}"
    try:
        chain = llm_chat_json(model=LIGHT_MODEL, max_tokens=300) | JsonOutputParser()
        questions = chain.invoke([{"role":"system","content":sys}, {"role":"user","content":user}])
        if isinstance(questions, list) and len(questions) > 0:
            return questions[:3]
    except Exception: pass
    return ["ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ 3ê°€ì§€ë¡œ ìš”ì•½í•´ì¤˜", "ì£¼ìš” ë¦¬ìŠ¤í¬ë‚˜ ìš°ë ¤ë˜ëŠ” ì ì€ ë¬´ì—‡ì´ì•¼?", "ì´ ë¬¸ì„œê°€ ì œì‹œí•˜ëŠ” í–¥í›„ ê³„íšì€ ë­ì•¼?"]
# =========================
# AI ì‹œê°í™” ê¸°ëŠ¥
# =========================
def generate_visualization_code(question: str, context: str) -> dict:
    sys = """
You are a data visualization expert. Your task is to generate Altair chart code based on a user's question and a provided text context.
1. First, determine if the question can be answered with a chart using the given context. Look for numerical data, trends, comparisons, or proportions.
2. If visualization is not possible or the data is insufficient, return a JSON object with a single key: `{"error": "No suitable data for visualization."}`.
3. If visualization is possible:
    a. Extract the necessary data and structure it as a JSON list of objects.
    b. Write Python code using the Altair library to create a chart. The code must use a pandas DataFrame named `df` which will be created from the data.
    c. Choose the best chart type (bar, line, pie, area, etc.) to answer the question. Make sure chart labels and titles are in KOREAN.
4. Return a single valid JSON object with two keys: "data" (the extracted data) and "code" (the Altair code string).
"""
    user = f"USER_QUESTION: \"{question}\"\n\nCONTEXT:\n{context}"
    try:
        chain = llm_chat_json(max_tokens=800) | JsonOutputParser()
        result = chain.invoke([{"role":"system","content":sys}, {"role":"user","content":user}])
        return result
    except Exception:
        return {"error": "Failed to generate visualization code."}

def safe_execute_altair_code(code_str: str, data: list):
    if not data or not isinstance(data, list): return None
    try:
        df = pd.DataFrame(data)
        local_scope = {"alt": alt, "pd": pd, "df": df}
        exec(code_str, {}, local_scope)
        chart = local_scope.get("chart")
        if chart and isinstance(chart, alt.TopLevelMixin):
            return chart
        return None
    except Exception: return None
# =========================
# ìƒíƒœ ì´ˆê¸°í™”
# =========================
if "vs" not in st.session_state: st.session_state.vs = None
if "pages" not in st.session_state: st.session_state.pages = None
if "messages" not in st.session_state: st.session_state.messages = []
if "uploaded_name" not in st.session_state: st.session_state.uploaded_name = None
if "suggested_questions" not in st.session_state: st.session_state.suggested_questions = []

# =========================
# ë©”ì¸ UI
# =========================
uploaded = st.file_uploader("1. ë¶„ì„í•  PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["pdf"])
use_visualization = st.checkbox("2. ë‹µë³€ ì‹œê°í™” ê¸°ëŠ¥ ì‚¬ìš© (ë² íƒ€)", value=False, help="ë‹µë³€ì— í‘œë‚˜ ìˆ˜ì¹˜ê°€ ìˆì„ ê²½ìš° AIê°€ ì°¨íŠ¸ë¥¼ í•¨ê»˜ ìƒì„±í•©ë‹ˆë‹¤.")

if uploaded:
    if st.session_state.uploaded_name != uploaded.name:
        with st.spinner("PDF ë¶„ì„ ë° ì¸ë±ì‹± ì¤‘..."):
            vs, pages = load_or_build_index(uploaded.read(), CHUNK_SIZE, CHUNK_OVERLAP, OPENAI_API_KEY)
            st.session_state.vs, st.session_state.pages = vs, pages
            st.session_state.uploaded_name = uploaded.name
            st.session_state.messages = []
            st.session_state.suggested_questions = generate_question_suggestions(pages)
        st.success(f"'{uploaded.name}' ë¶„ì„ ì™„ë£Œ! ì•„ë˜ ì¶”ì²œ ì§ˆë¬¸ì„ í´ë¦­í•˜ê±°ë‚˜ ì§ì ‘ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
else:
    st.session_state.vs, st.session_state.pages, st.session_state.uploaded_name, st.session_state.messages, st.session_state.suggested_questions = None, None, None, [], []

# =========================
# ëŒ€í™”í˜• ì²´ì¸ ë¹Œë”
# =========================
def build_chain(retriever):
    system = "ë„ˆëŠ” ì—…ë¡œë“œëœ PDFì— ê·¼ê±°í•´ í•œêµ­ì–´ë¡œ ë‹µí•œë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, í˜„ì¬ ì§ˆë¬¸ì— ê°€ì¥ ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•œë‹¤. í•­ìƒ **ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ì™€ ê°œìˆ˜ ìš”êµ¬ë¥¼ ì •í™•íˆ ë”°ë¥¸ë‹¤**. ê°€ëŠ¥í•˜ë©´ bulletì„ ì‚¬ìš©í•˜ê³ , ê° í•µì‹¬ ì£¼ì¥ ëì— [p.í˜ì´ì§€] ê·¼ê±°ë¥¼ ë¶™ì¸ë‹¤. ë¬¸ì„œì™€ ë¬´ê´€í•œ ì¶”ì¸¡ì€ ê¸ˆì§€í•œë‹¤."
    prompt = ChatPromptTemplate.from_messages([
        ("system", system + "\n\nCONTEXT:\n{context}"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}")
    ])
    chain = (
        RunnablePassthrough.assign(context=itemgetter("question") | retriever | _format_docs)
        | {"answer": prompt | llm_chat(max_tokens=MAX_TOKENS, temperature=TEMPERATURE) | StrOutputParser(), "context": itemgetter("context")}
    )
    return chain
# =========================
# ì§ˆë¬¸ ì²˜ë¦¬ ë¡œì§ í•¨ìˆ˜
# =========================
def handle_question(question: str):
    st.session_state.messages.append({"role": "user", "content": question})
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            base_retriever = st.session_state.vs.as_retriever(search_type="mmr", search_kwargs={"k": TOP_K, "fetch_k": FETCH_K})
            retriever = base_retriever
            if USE_COMPRESSION:
                compressor = LLMChainExtractor.from_llm(llm_light(max_tokens=300, temperature=0))
                retriever = ContextualCompressionRetriever(base_retriever=base_retriever, base_compressor=compressor)

            chain = build_chain(retriever)
            chat_history = [HumanMessage(content=msg["content"]) if msg["role"] == "user" else AIMessage(content=msg["content"]) for msg in st.session_state.messages[:-1]]
            result = chain.invoke({"question": question, "chat_history": chat_history})
            answer = result.get("answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            context = result.get("context", "")

            if not (answer or "").strip(): answer = "âš ï¸ ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."
            st.write(answer)

            if use_visualization:
                with st.spinner("ì‹œê°í™” ìƒì„± ì¤‘..."):
                    viz_result = generate_visualization_code(question, context)
                    if "error" not in viz_result:
                        chart = safe_execute_altair_code(viz_result.get("code"), viz_result.get("data"))
                        if chart: st.altair_chart(chart, use_container_width=True)
                        else: st.info("ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    else: st.info(f"ì‹œê°í™” ì •ë³´: {viz_result['error']}")

            cited_pages = extract_page_citations(answer)
            if cited_pages and st.session_state.pages:
                with st.expander(f"ğŸ” ì¸ìš©ëœ í˜ì´ì§€ ìŠ¤ë‹ˆí« ë³´ê¸° ({len(cited_pages)}ê°œ)"):
                    page_map = {pg: txt for pg, txt in st.session_state.pages}
                    for pg in cited_pages: st.markdown(f"**[p.{pg}]**\n\n{(page_map.get(pg) or '')[:500]}")
    st.session_state.messages.append({"role": "assistant", "content": answer})

# =========================
# Q&A ë° ëŒ€í™” ê¸°ë¡ UI
# =========================
if st.session_state.vs:
    st.markdown("---")
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if st.session_state.suggested_questions and len(st.session_state.messages) == 0:
        st.markdown("##### AI ì¶”ì²œ ì§ˆë¬¸:")
        cols = st.columns(len(st.session_state.suggested_questions))
        for i, question_prompt in enumerate(st.session_state.suggested_questions):
            with cols[i]:
                if st.button(question_prompt, key=f"suggestion_{i}", use_container_width=True):
                    handle_question(question_prompt)
                    st.rerun()
    
    if user_input := st.chat_input("PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
        handle_question(user_input)
        st.rerun()

else:
    st.info("PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ AI ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")